{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThermoGan\n",
    "\n",
    "**Author** - [Lukas Vlcek](https://https://github.com/vlcekl)\n",
    "\n",
    "**thermoGAN** is designed keeping a high degree of extensibility in mind, and allows you to write custom loss functions of your own without having to rewrite the entire training and evaluation loop. This can be done by extending the ```torchgan.losses.GeneratorLoss``` or the ```torchgan.losses.DiscriminatorLoss``` object. \n",
    "\n",
    "All **TorchGAN** losses have a ```train_ops``` associated with it that dictates what steps are to be followed for the loss to be computed and backpropagated. By default, most of the ```train_ops``` follow a **Two Timescale Update Rule (TTUR) ** as follows\n",
    "\n",
    "1. Sample a noise vector from a Normal Distribution $z \\sim \\mathcal{N}(0,\\,1)$\n",
    "3. $d_{real} = D(x)$\n",
    "4. $d_{fake} = D(G(z))$\n",
    "5. $\\mathcal{L} = Loss(d_{real}, d_{fake})$  (*for a Generator Loss $d_{real}$ is generally not computed*)\n",
    "6. Backpropagate over $\\mathcal{L}$\n",
    "\n",
    "Where \n",
    "* $x$ is a sample from the Data Distribution\n",
    "* $D$ is the Discriminator Network\n",
    "* $G$ is the Generator Network\n",
    "\n",
    "Simple losses that conform to this kind of an update rule can be easily implemented by overriding the ```forward``` method of the ```GeneratorLoss``` or ```DiscriminatorLoss``` object\n",
    "\n",
    "** *NB: It is highly recommended that you go over the Introduction to TorchGAN Tutorial before reading this* *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING CUSTOM LOSSES THAT FOLLOW THE STANDARD UPDATE RULE\n",
    "\n",
    "We shall demonstrate this by implementing the [Boundary Seeking GAN by Hjelm et. al.](https://arxiv.org/abs/1702.08431), also known as BGAN\n",
    "BGAN involves a departure from the Minimax Loss by changing the Generator Loss term in the following manner\n",
    "\n",
    "$$ \\mathcal{L_{generator}} = \\frac{1}{2}E_{z \\sim p(z)}[(log(D(G(z))) - log(1 - D(G(z))))^2]$$\n",
    "\n",
    "where\n",
    "\n",
    "* $z$ is the noise sampled from a probability distribution\n",
    "* $D$ is the Discriminator Network\n",
    "* $G$ is the Generator Network\n",
    "\n",
    "We can observe that the update rule for such a loss confirms with the Standard Update Rule used in **TorchGAN**, hence this loss can be implemented simply by extending the ```torchgan.losses.GeneratorLoss``` object and overriding the ```forward``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchgan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-deb269717e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Torchgan Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchgan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneratorLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinimaxDiscriminatorLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchgan'"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Pytorch and Torchvision Imports\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Torchgan Imports\n",
    "#import torchgan\n",
    "from torchgan.losses import GeneratorLoss, MinimaxDiscriminatorLoss\n",
    "from torchgan.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad, depad\n",
    "\n",
    "# My source code for TGAN model\n",
    "sys.path.append('../src')\n",
    "from tgan.model import TGANGenerator, TGANDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "print(\"Random Seed: \", manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 2D Ising model configurations generated in MC simulation\n",
    "\n",
    "1. The size of the configurations is assumed (k, k) with k power of 2\n",
    "2. Currently uses 4x4 kernel with no padding (in future it should be 3x3 kernel with cyclic padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "sim_dir = '../sim/maxi'\n",
    "temps = ['t1', 't2', 't5', 't10', 't20', 't100']  # ferromagnetic\n",
    "temps.extend(['a1', 'a2', 'a5', 'a10', 'a20', 'a100'])  # antiferromagnetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xyz_lattice(filename, type_map=None):\n",
    "    \"\"\"\n",
    "    Reads lattice xyz file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "              full path and name of the xyz file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    latt: ndarray(n, lx, ly, lz)\n",
    "          trajectory n lattice configurations of dimensions (lx,ly,lz)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'r') as fc:\n",
    "\n",
    "        cfgs = []\n",
    "\n",
    "        for line in iter(fc.readline, ''):\n",
    "            nat = int(line.strip().split()[0])\n",
    "\n",
    "            box = tuple(map(int, fc.readline().strip().split()))[0:3]\n",
    "            cfg = np.zeros(box, dtype=int)\n",
    "            \n",
    "            # fill lattice with atom types\n",
    "            for i in range(nat):\n",
    "                sarr = fc.readline().strip().split()\n",
    "                ti = int(sarr[0])\n",
    "                x, y, z = [int(x) for x in sarr[1:4]]\n",
    "                if type_map:\n",
    "                    cfg[x-1, y-1, z-1] = type_map[ti]\n",
    "                else:\n",
    "                    cfg[x-1, y-1, z-1] = ti\n",
    "\n",
    "            cfgs.append(cfg)\n",
    "\n",
    "    return cfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local library for data processing\n",
    "\n",
    "class IsingDataset(Dataset):\n",
    "    \"\"\" Dataset class sublcassed from torch's dataset utility\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        path to directory with class folders\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, traj_file, labels=None, start=0, end=None, label_encode='onehot'):\n",
    "\n",
    "        if labels is None:\n",
    "            labels = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "            \n",
    "        all_cfgs = []\n",
    "        all_labels = []\n",
    "        for label in labels:\n",
    "            cfg_dir = os.path.join(path, label)\n",
    "            cfgs = read_xyz_lattice(os.path.join(cfg_dir, traj_file), type_map={-1:0, 1:1})\n",
    "            # Take only one layer perpendicular to z-direction (Assumes 2D model)\n",
    "            all_cfgs.extend([cfg[:,:,0] for cfg in cfgs[start:end]])\n",
    "            all_labels.extend([label]*len(cfgs[start:end]))\n",
    "        \n",
    "        self.X = torch.FloatTensor(np.array(all_cfgs))\n",
    "        \n",
    "        # One-hot encoding of the target labels\n",
    "        if label_encode == 'onehot':\n",
    "            self.y = self._get_onehot(all_labels)\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(np.array(all_labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "    def _get_onehot(self, labels):\n",
    "        labels = list(labels)\n",
    "        batch_size = len(labels)\n",
    "        unique_labels = sorted(list(set(labels)))\n",
    "        n_classes = len(unique_labels)\n",
    "    \n",
    "        # convert labels to numbers from 0 to n_classes-1\n",
    "        labels_id = [unique_labels.index(label) for label in labels]\n",
    "        labels_id = np.reshape(labels_id, (batch_size, -1))\n",
    "        labels_id = torch.LongTensor(labels_id)\n",
    "    \n",
    "        onehot = torch.FloatTensor(batch_size, n_classes)\n",
    "        onehot.zero_()\n",
    "        onehot.scatter_(1, labels_id, 1)\n",
    "        return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 (tensor([[[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "         [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "         [1., 0., 1.,  ..., 1., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 1.],\n",
      "         [1., 1., 1.,  ..., 0., 1., 0.],\n",
      "         [0., 1., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "         [1., 0., 1.,  ..., 0., 1., 0.],\n",
      "         [1., 1., 0.,  ..., 1., 0., 1.]]]), tensor([[1.],\n",
      "        [1.]]))\n"
     ]
    }
   ],
   "source": [
    "dataset = IsingDataset(sim_dir, traj_file='lg.xyz', labels=['a2'], start=1001)\n",
    "print('a2', dataset[3000:3002])\n",
    "# print('a5', dataset[7000:7002])\n",
    "# print('t1', dataset[11000:11002])\n",
    "# print('t10', dataset[15000:15002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, torch.Size([32, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples 4000\n",
      "Minibatches: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b004ac8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "print(\"Samples\", len(dataset))\n",
    "print(\"Minibatches:\", len(dataloader))\n",
    "# for batch in dataloader:\n",
    "#     print(batch[0].shape)\n",
    "    \n",
    "plt.matshow(dataset[-2000][0], cmap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = {\n",
    "    'encoding_dims': feature_dim,\n",
    "    'out_size': dataset[0][0].shape[0],\n",
    "    'out_channels': 1,\n",
    "    'step_channels': 2,\n",
    "    'batchnorm': False,\n",
    "    'last_nonlinearity': nn.Sigmoid()\n",
    "}\n",
    "\n",
    "netG = TGANGenerator(**generator_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_params = {\n",
    "    'decoding_dims': feature_dim,\n",
    "    'in_size': dataset[0][0].shape[0],\n",
    "    'in_channels': 1,\n",
    "    'step_channels': 2,\n",
    "    'batchnorm': False,\n",
    "    'last_nonlinearity': nn.Sigmoid()\n",
    "}\n",
    "\n",
    "netD = TGANDiscriminator(**discriminator_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run on CPU\n"
     ]
    }
   ],
   "source": [
    "# Device setup \n",
    "if torch.cuda.is_available():\n",
    "    on_cuda = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(\"Will run on GPU\")\n",
    "else:\n",
    "    on_cuda = False\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Will run on CPU\")\n",
    "    \n",
    "negG = netG.to(device)\n",
    "negD = netD.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== GENERATOR =========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TGANGenerator(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): CircularDepad2d((0, 1, 0, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): CircularDepad2d((0, 1, 0, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(4, 2, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): CircularDepad2d((0, 1, 0, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(2, 1, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): CircularDepad2d((0, 1, 0, 1))\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('======== GENERATOR =========')\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== DISCRIMINATOR =========\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TGANDiscriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): CircularPad2d((0, 1, 0, 1))\n",
       "      (1): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): CircularPad2d((0, 1, 0, 1))\n",
       "      (1): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): CircularPad2d((0, 1, 0, 1))\n",
       "      (1): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): CircularPad2d((0, 1, 0, 1))\n",
       "      (1): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('======== DISCRIMINATOR =========')\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function setup\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer setup\n",
    "lr = 0.001\n",
    "betas = (0.5, 0.999)\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=betas)\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch setup\n",
    "if on_cuda:\n",
    "    n_epochs = 100\n",
    "else:\n",
    "    n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-619fd71f9d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetD\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "# Auxiliary data \n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        #######################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        #######################\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # train with real\n",
    "        real_data = data[0].to(device)\n",
    "        output = netD(real_data)\n",
    "\n",
    "        batch_size = real_data.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        errD_real = loss(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, feature_dim, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        output = netD(fake.detach())\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        errD_fake = loss(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        # Total loss and optimize D\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        #######################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        #######################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        output = netD(fake)\n",
    "\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        \n",
    "        errG = loss(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        # optimize G\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, opt.niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % opt.outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING THE BOUNDARY SEEKING LOSS\n",
    "\n",
    "As discussed above, the Boundary Seeking Loss is implementing by overriding the ```forward``` pass of the Generator Loss without requiring any modifications to the ```train_ops```. \n",
    "\n",
    "The ```forward``` method receives the object $d_{fake} = D(G(z))$ as a parameter, where $G$ is the Generator Network, $D$ is the Discriminator Network and $z$ is a sample from the Noise Prior.\n",
    "\n",
    "\n",
    "*NB: This example shall be using the standard DCGAN Generator and Discriminator available in ```torchgan.models```. By default, the last layer of the discriminator does not apply a Sigmoid nonlinearity, the reasson for which has already been discussed in the **Introduction to TorchGAN** tutorial. As a result, the nonlinearity is applied within the loss by a call to ```torch.sigmoid```. One can also alternatively omit this and set the ```last_nonlinearity``` property of the DCGAN Discriminator to ```torch.nn.Sigmoid``` *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundarySeekingLoss(GeneratorLoss):\n",
    "    def forward(self, dx):\n",
    "        dx = torch.sigmoid(dx)\n",
    "        return 0.5 * torch.mean((torch.log(dx) - torch.log(1.0 - dx)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the paper, only the Generator Loss is modified. Hence we will use one of the predefined losses, **MinimaxDiscriminatorLoss** for the Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [BoundarySeekingLoss(), MinimaxDiscriminatorLoss()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CONFIGURATION\n",
    "\n",
    "We shall now be defining the neural networks for the discriminator and generator and also set up their respective optimizers. For understanding how to do this please refer to the previous set of tutorials.\n",
    "\n",
    "It should be noted that we have modified the Discriminator Output to use a **nn.Sigmoid** to conform with our Loss Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config = {\n",
    "    \"generator\": {\"name\": DCGANGenerator,\n",
    "                  \"args\": {\"out_channels\": 1, \"step_channels\": 8},\n",
    "                  \"optimizer\": {\n",
    "                      \"name\": Adam,\n",
    "                      \"args\": {\"lr\": 0.0001, \"betas\": (0.5, 0.999)}\n",
    "                  }\n",
    "                 },\n",
    "    \"discriminator\": {\"name\": DCGANDiscriminator,\n",
    "                      \"args\": {\"in_channels\": 1, \"step_channels\": 8},\n",
    "                      \"optimizer\": {\n",
    "                          \"name\": Adam,\n",
    "                          \"args\": {\"lr\": 0.0001, \"betas\": (0.5, 0.999)}\n",
    "                      }\n",
    "                     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE THE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING BGAN\n",
    "\n",
    "Now we shall start the training. First we need to create the **Trainer** object. When creating this object all the necessary neural nets and their optimizers get instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(network_config, losses, ncritic=5, epochs=epochs, sample_size=64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZING THE GENERATED IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(plt.imread(\"{}/epoch{}_generator.png\".format(trainer.recon, 280)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
